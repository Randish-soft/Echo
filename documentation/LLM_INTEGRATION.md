# Echo - Local LLM Integration Guide

Echo now uses a local LLM (Large Language Model) to generate comprehensive, context-aware documentation from your GitHub repositories.

## Overview

The system uses **Ollama** running locally to generate documentation without sending your code to external services. All processing happens on your machine.

## Architecture

```
┌─────────────┐     ┌──────────────┐     ┌─────────────┐
│   Frontend  │────▶│  C++ Backend │────▶│   Ollama    │
│  (Next.js)  │     │   (Crow API) │     │    (LLM)    │
└─────────────┘     └──────────────┘     └─────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  PostgreSQL  │
                    │  (Metadata)  │
                    └──────────────┘
```

## Setup Instructions

### 1. Start All Services

```bash
docker-compose up -d
```

This will start:
- PostgreSQL database
- Ollama LLM service
- C++ backend server
- Next.js frontend

### 2. Pull the LLM Model

After services are running, pull the required model:

```bash
./setup-ollama.sh
```

This downloads the `llama3.1:8b` model (~4.7GB). First-time setup takes 5-10 minutes depending on your internet connection.

### 3. Verify Installation

Check that Ollama is working:

```bash
docker exec -it echo_ollama ollama list
```

You should see `llama3.1:8b` in the list.

## Documentation Types

### Internal Documentation (For Developers & Technical Teams)

1. **API Documentation**
   - REST endpoints with parameters
   - Request/response schemas
   - Authentication details
   - Example requests

2. **Database Documentation**
   - Table schemas and relationships
   - Indexes and constraints
   - Migration history
   - Data models

3. **Architecture Documentation**
   - System design overview
   - Component interactions
   - Design patterns used
   - Architectural decisions

4. **Developer Onboarding**
   - Environment setup
   - Development workflow
   - Coding standards
   - First contribution guide

5. **Code Conventions**
   - Style guidelines
   - Naming conventions
   - Best practices
   - Code review standards

6. **Technical Specification**
   - Detailed technical specs
   - Requirements and constraints
   - Foundation for formal docs

### External Documentation (For End Users & Stakeholders)

1. **User Manual**
   - Feature usage guides
   - Step-by-step instructions
   - Common workflows
   - User-friendly language

2. **Installation Guide**
   - System requirements
   - Installation steps
   - Configuration
   - Troubleshooting

3. **FAQ**
   - Common questions
   - Quick answers
   - Organized by category

4. **Troubleshooting Guide**
   - Common issues and solutions
   - Error messages explained
   - Diagnostic steps

5. **Release Notes**
   - New features
   - Bug fixes
   - Breaking changes
   - Upgrade instructions

6. **Integration Guide**
   - How to integrate with other systems
   - API usage examples
   - Authentication setup
   - Webhooks and callbacks

## Usage

1. **Add Repository**: Provide a GitHub URL
2. **Select Repository**: Choose from indexed repos
3. **Choose Documentation Type**: Select from 12 specialized types
4. **Select Audience**: Developers, Users, Managers, etc.
5. **Generate**: Wait 30-60 seconds for AI generation
6. **Edit & Export**: Edit markdown, download as PDF

## Model Options

You can use different models based on your needs:

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| llama3.1:8b | 4.7GB | Medium | High | General docs (default) |
| mistral:7b | 4GB | Fast | Good | Quick generation |
| codellama:7b | 4GB | Fast | High | Code-heavy docs |
| deepseek-coder:6.7b | 4GB | Fast | Excellent | Technical docs |

### Change Model

To use a different model:

1. Pull the model:
```bash
docker exec -it echo_ollama ollama pull mistral:7b
```

2. The backend will automatically use the configured model (default: `llama3.1:8b`).

To permanently change the default model, update `LLMService.h`:
```cpp
model_name = "mistral:7b";  // Change this line
```

## Performance

- **First generation**: 30-60 seconds (model warming up)
- **Subsequent generations**: 20-40 seconds
- **Memory usage**: ~6-8GB RAM for 8B parameter models
- **GPU acceleration**: Automatically used if available

## Compliance & Legal Notice

⚠️ **Important**: Documentation generated by AI is for **operational use only**.

For formal regulatory submissions (ISO certifications, EU Commission reports, etc.), this AI-generated documentation must be:
- Reviewed by qualified personnel
- Verified for accuracy
- Approved by appropriate authorities
- Properly attributed in terms of authorship

The system clearly labels all AI-generated content with appropriate disclaimers.

## Troubleshooting

### Ollama not responding

```bash
docker restart echo_ollama
docker logs echo_ollama
```

### Model not found

```bash
docker exec -it echo_ollama ollama pull llama3.1:8b
```

### Slow generation

- Use a smaller model (mistral:7b)
- Ensure you have enough RAM (8GB+ recommended)
- Check CPU usage during generation

### Backend can't reach Ollama

Check the network:
```bash
docker exec -it echo_backend ping ollama
```

Verify environment variable:
```bash
docker exec -it echo_backend env | grep OLLAMA_HOST
```

Should show: `OLLAMA_HOST=http://ollama:11434`

## Development

### Adding New Documentation Types

1. Add system prompt in `PromptTemplates.h`:
```cpp
{"new_doc_type", "You are an expert in..."}
```

2. Add specific instructions in `getSpecificInstructions()`:
```cpp
{"new_doc_type", "## Instructions:\n1. ..."}
```

3. Add mapping in `DocumentationService.h`:
```cpp
{"frontend_new_doc", "new_doc_type"}
```

4. Update frontend `DocGenerator.jsx`:
```javascript
{ value: "frontend_new_doc", label: "New Doc Type", desc: "Description" }
```

### Testing Ollama Directly

Test the LLM without the full stack:

```bash
docker exec -it echo_ollama ollama run llama3.1:8b "Explain what a REST API is"
```

## Resource Requirements

**Minimum:**
- 8GB RAM
- 10GB disk space
- 2 CPU cores

**Recommended:**
- 16GB RAM
- 20GB disk space (for multiple models)
- 4+ CPU cores
- GPU (optional, for faster generation)

## Privacy & Security

✅ **All processing is local** - No data sent to external services
✅ **Your code stays on your machine** - Complete privacy
✅ **No API keys required** - No vendor lock-in
✅ **Fully offline capable** - Works without internet (after model download)

## License

This LLM integration uses Ollama (MIT License) and open-source models with permissive licenses suitable for commercial use.
